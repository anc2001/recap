{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":49596,"status":"ok","timestamp":1683594207956,"user":{"displayName":"collab usage7","userId":"12719994298090451758"},"user_tz":240},"id":"8549bmbUnzbA","outputId":"78d82892-5ef3-4d17-99a8-4c1696f5b1d8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ftfy\n","  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n","Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.6)\n","Installing collected packages: ftfy\n","Successfully installed ftfy-6.1.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-gc1hut4h\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-gc1hut4h\n","  Resolved https://github.com/openai/CLIP.git to commit a9b1bf5920416aaeaec965c25dd9e8f98c864f16\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.1.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.65.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.0.0+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.15.1+cu118)\n","Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.12.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.11.1)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.0.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.5.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (16.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.27.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.22.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (8.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.2)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2022.12.7)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n","Building wheels for collected packages: clip\n","  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369398 sha256=c6bb9ed118a9053f5bcb2b109ddad2db5be2f5ec97a03ef767144c81433c34cd\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-3u025mwc/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n","Successfully built clip\n","Installing collected packages: clip\n","Successfully installed clip-1.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pdbpp\n","  Downloading pdbpp-0.10.3-py2.py3-none-any.whl (23 kB)\n","Collecting wmctrl\n","  Downloading wmctrl-0.4.tar.gz (5.4 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from pdbpp) (2.14.0)\n","Collecting fancycompleter>=0.8\n","  Downloading fancycompleter-0.9.1-py3-none-any.whl (9.7 kB)\n","Collecting pyrepl>=0.8.2\n","  Downloading pyrepl-0.9.0.tar.gz (48 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.7/48.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: wmctrl, pyrepl\n","  Building wheel for wmctrl (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wmctrl: filename=wmctrl-0.4-py3-none-any.whl size=3856 sha256=0a62a8168b2b7cc20a457531d1a3a97703c8760dfec6ea6d786b6f30ea65d39c\n","  Stored in directory: /root/.cache/pip/wheels/0d/fc/49/e978b317217e03bb241c860682bff16775ee7e298eb00230c7\n","  Building wheel for pyrepl (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyrepl: filename=pyrepl-0.9.0-py3-none-any.whl size=59907 sha256=ae2295db798558f83dd28cb51495ef5fdabdf77b374cb67704d3ce3d31f2e4e3\n","  Stored in directory: /root/.cache/pip/wheels/68/8f/52/5191cdf3a2599696725a5245439fe59d4af6de3bc34edba47e\n","Successfully built wmctrl pyrepl\n","Installing collected packages: wmctrl, pyrepl, fancycompleter, pdbpp\n","Successfully installed fancycompleter-0.9.1 pdbpp-0.10.3 pyrepl-0.9.0 wmctrl-0.4\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pdb"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pycocoevalcap\n","  Downloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from pycocoevalcap) (2.0.6)\n","Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->pycocoevalcap) (3.7.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->pycocoevalcap) (1.22.4)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (2.8.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (23.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (3.0.9)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.4.4)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (4.39.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.0.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (0.11.0)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (8.4.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.16.0)\n","Installing collected packages: pycocoevalcap\n","Successfully installed pycocoevalcap-1.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting PyDictionary\n","  Downloading PyDictionary-2.0.1-py3-none-any.whl (6.1 kB)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from PyDictionary) (8.1.3)\n","Collecting bs4\n","  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting goslate\n","  Downloading goslate-1.5.4.tar.gz (14 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from PyDictionary) (2.27.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4->PyDictionary) (4.11.2)\n","Collecting futures\n","  Downloading futures-3.0.5.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->PyDictionary) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->PyDictionary) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->PyDictionary) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->PyDictionary) (3.4)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4->PyDictionary) (2.4.1)\n","Building wheels for collected packages: bs4, goslate, futures\n","  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1270 sha256=a0992973b73380fad6e010aeff6b188248ca9ac8d7512a947d598a0170de07f0\n","  Stored in directory: /root/.cache/pip/wheels/25/42/45/b773edc52acb16cd2db4cf1a0b47117e2f69bb4eb300ed0e70\n","  Building wheel for goslate (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for goslate: filename=goslate-1.5.4-py3-none-any.whl size=11594 sha256=792709d531b5fb87e4baba43cbd15b6c4b928f255cc9f924bb4a7632b22908ae\n","  Stored in directory: /root/.cache/pip/wheels/b5/30/e9/63b6de83667be2977ee793a146a2c80f8e588d5c0203b39dc9\n","  Building wheel for futures (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for futures: filename=futures-3.0.5-py3-none-any.whl size=14087 sha256=926e7644b0ba536aaffadaa6f53e56a44937535391d4c5cd02d405e19422149c\n","  Stored in directory: /root/.cache/pip/wheels/ef/af/93/48739d464ba97d4cdc77c627d282f9794c8d276e42aaa92160\n","Successfully built bs4 goslate futures\n","Installing collected packages: futures, goslate, bs4, PyDictionary\n","Successfully installed PyDictionary-2.0.1 bs4-0.0.1 futures-3.0.5 goslate-1.5.4\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["concurrent"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting nltk==3.6.5\n","  Downloading nltk-3.6.5-py3-none-any.whl (1.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.6.5) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.6.5) (1.2.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk==3.6.5) (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk==3.6.5) (4.65.0)\n","Installing collected packages: nltk\n","  Attempting uninstall: nltk\n","    Found existing installation: nltk 3.8.1\n","    Uninstalling nltk-3.8.1:\n","      Successfully uninstalled nltk-3.8.1\n","Successfully installed nltk-3.6.5\n","ERROR: unknown command \"inSstall\" - maybe you meant \"install\"\n"]}],"source":["! pip install ftfy regex tqdm\n","! pip install git+https://github.com/openai/CLIP.git\n","! pip install pdbpp\n","! pip install pycocoevalcap\n","! pip install PyDictionary\n","! pip install nltk==3.6.5\n","! pip inSstall torch"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20537,"status":"ok","timestamp":1683594228489,"user":{"displayName":"collab usage7","userId":"12719994298090451758"},"user_tz":240},"id":"4mVFUHaPrkdP","outputId":"b4acac49-265c-41f3-b991-c80d309bbbba"},"outputs":[{"output_type":"stream","name":"stdout","text":["update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java to provide /usr/bin/java (java) in manual mode\n","openjdk version \"1.8.0_362\"\n","OpenJDK Runtime Environment (build 1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09)\n","OpenJDK 64-Bit Server VM (build 25.362-b09, mixed mode)\n"]}],"source":["import locale\n","locale.getpreferredencoding = lambda: \"UTF-8\"\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","!update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n","!java -version"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"3jhdkGc1o3x3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683594250601,"user_tz":240,"elapsed":22115,"user":{"displayName":"collab usage7","userId":"12719994298090451758"}},"outputId":"05c5f52e-e3b1-496a-9dc3-e49bf60a4c5a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]}],"source":["from pycocoevalcap.cider.cider import Cider\n","from pycocoevalcap.spice.spice import Spice\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","import nltk\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","\n","from nltk.translate import meteor_score\n","from nltk.tokenize import word_tokenize\n","from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n","import numpy as np\n","import torch\n","from PIL import Image\n","import pandas as pd\n","import random\n","import copy\n","import os\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"K-3HJUnvg8zs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683594276499,"user_tz":240,"elapsed":25912,"user":{"displayName":"collab usage7","userId":"12719994298090451758"}},"outputId":"ef0f1a06-1b53-4c05-9625-ee385180f22a"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|████████████████████████████████████████| 890M/890M [00:06<00:00, 136MiB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Model parameters: 427,616,513\n","Input resolution: 224\n","Context length: 77\n","Vocab size: 49408\n"]}],"source":["import clip\n","\n","# RN50 will be fastest, while ViT-L will be the most performant.\n","clip.available_models()\n","\n","\n","clip_backbones = {'ResNet50': \"RN50\", 'ResNet101': \"RN50x16\", 'ViTL': 'ViT-L/14', 'ViTB': \"ViT-B/16\", 'ViTB32': 'ViT-B/32'}\n","\n","chosen_model = 'ViTL'\n","\n","clip_model, preprocessor = clip.load(clip_backbones[chosen_model])\n","clip_model.cuda().eval()\n","\n","input_resolution = clip_model.visual.input_resolution\n","context_length = clip_model.context_length\n","vocab_size = clip_model.vocab_size\n","\n","print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in clip_model.parameters()]):,}\")\n","print(\"Input resolution:\", input_resolution)\n","print(\"Context length:\", context_length)\n","print(\"Vocab size:\", vocab_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":74209,"status":"ok","timestamp":1683589859066,"user":{"displayName":"Shreyas Sundara Raman","userId":"16214783268136716971"},"user_tz":240},"id":"GvLmG1JrpT3S","outputId":"acac82d9-0680-46be-dc37-a84d676b83f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading stanford-corenlp-3.6.0 for SPICE ...\n","Progress: 384.5M / 384.5M (100.0%)\n","Extracting stanford-corenlp-3.6.0 ...\n","Done.\n"]}],"source":["cider_metric = Cider()\n","spice_metric = Spice()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bbUlzcGTpUnA"},"outputs":[],"source":["def get_weights(max_n=1):\n","\n","  if max_n==1:\n","    weights = (1, 0, 0, 0)\n","  elif max_n == 2:\n","    weights = (0.5, 0.5, 0, 0)\n","  elif max_n == 3:\n","    weights = (0.33, 0.33, 0.33, 0)\n","  else:\n","    weights = (0.25, 0.25, 0.25, 0.25)\n","\n","  return weights \n","\n","def compute_all_bleu_scores(caption_dictionary, experiment='test'):\n","  \n","  bleu_df = pd.read_csv('gdrive/Shareddrives/CS2952N/human_preference.csv')\n","  \n","  avg_bleu_scores = []\n","\n","  all_hypoth_captions = []\n","  all_gt_captions = []\n","\n","  all_bleu1 = []\n","  all_bleu2 = []\n","  all_bleu3 = []\n","  all_bleu4 = []\n","\n","  references = []\n","  hypotheses = []\n","\n","  for idx, row in bleu_df.iterrows():\n","\n","    if row['image_filename'] not in expert_annotations:\n","      continue\n","\n","    hypoth_caption = caption_dictionary[row['target_caption']][2]\n","    gt_captions = caption_dictionary[row['gt_captions']]\n","\n","    all_hypoth_captions.append(hypoth_caption)\n","    all_gt_captions.append(';'.join(gt_captions))\n","\n","\n","    hypotheses.append(hypoth_caption.split(' '))\n","    ref_for_i = []\n","\n","    for i in range(len(gt_captions)):\n","\n","      ref_for_i.append(gt_captions[i].split(' '))\n","      \n","    references.append(ref_for_i)\n","\n","  \n","  for n in range(1,5):\n","   \n","    weights = get_weights(n)\n","\n","\n","    for ref,hyp in zip(references, hypotheses):\n","      bleu_score = sentence_bleu(ref, hyp, weights=weights)\n","\n","      if n==1:\n","        all_bleu1.append(bleu_score)\n","      elif n==2:\n","        all_bleu2.append(bleu_score)\n","      elif n==3:\n","        all_bleu3.append(bleu_score)\n","      elif n==4:\n","        all_bleu4.append(bleu_score)\n","\n","      \n","      \n","\n","    corp_bleu = corpus_bleu(references, hypotheses, weights = weights)\n","    avg_bleu_scores.append(corp_bleu)\n","  \n","\n","  bleu_df['bleu1_score'] = all_bleu1\n","  bleu_df['bleu2_score'] = all_bleu2\n","  bleu_df['bleu3_score'] = all_bleu3\n","  bleu_df['bleu4_score'] = all_bleu4\n","  bleu_df['target_caption_text'] = all_hypoth_captions\n","  bleu_df['gt_caption_text'] = all_gt_captions\n"," \n","  bleu_df.to_csv('gdrive/Shareddrives/CS2952N/human_preference.csv', index=False)\n","  \n","  return avg_bleu_scores \n","\n","def compute_meteor_score(gt_captions, pred_caption):\n","  \"\"\"\n","  pred_caption: string containing predicted caption\n","  gt_caption: string containing ground-truth caption\n","  \"\"\"\n","\n","  return max([meteor_score.single_meteor_score(word_tokenize(gt), word_tokenize(pred_caption)) for gt in gt_captions])\n","\n","\n","\n","def compute_all_meteor_scores(caption_dictionary, experiment='test'):\n","\n","  meteor_df = pd.read_csv('gdrive/Shareddrives/CS2952N/human_preference.csv')\n","  \n","\n","  avg_meteor = 0\n","  count = 0\n","\n","  all_meteor_scores = []\n","  all_hypoth_captions = []\n","  all_gt_captions = []\n","\n","  for idx, row in meteor_df.iterrows():\n","\n","    if row['image_filename'] not in expert_annotations:\n","      continue\n","\n","    hypoth_caption = caption_dictionary[row['target_caption']][2]\n","    gt_captions = caption_dictionary[row['gt_captions']]\n","\n","    all_hypoth_captions.append(hypoth_caption)\n","    all_gt_captions.append(';'.join(gt_captions))\n","\n","\n","    refs = []\n","    for i in range(len(gt_captions)):\n","\n","      refs.append(gt_captions[i])\n","\n","    score = compute_meteor_score(refs, hypoth_caption)\n","      \n","    all_meteor_scores.append(score)\n","\n","    avg_meteor += score\n","    count += 1\n","\n","  meteor_df['target_caption_text'] = all_hypoth_captions\n","  meteor_df['gt_caption_text'] = all_gt_captions\n","  meteor_df['meteor_score'] = all_meteor_scores\n","  meteor_df.to_csv('gdrive/Shareddrives/CS2952N/human_preference.csv', index=False)\n","  \n","  return avg_meteor/count\n","\n","\n","\n","def compute_all_spice_scores(caption_dictionary, experiment='test'):\n","  \n","  spice_df = pd.read_csv('gdrive/Shareddrives/CS2952N/human_preference.csv')\n","\n","  references = {}\n","  hypotheses = {}\n","\n","  all_spice_scores = []\n","  all_hypoth_captions = []\n","  all_gt_captions = []\n","\n","\n","  for idx, row in spice_df.iterrows():\n","\n","    if row['image_filename'] not in expert_annotations:\n","      continue\n","\n","    hypoth_caption = caption_dictionary[row['target_caption']][2]\n","    gt_captions = caption_dictionary[row['gt_captions']]\n","\n","    all_hypoth_captions.append(hypoth_caption)\n","    all_gt_captions.append(';'.join(gt_captions))\n","\n","\n","    hypotheses[row['target_caption']+'_'+row['gt_captions']] = [hypoth_caption]\n","    references[row['target_caption']+'_'+row['gt_captions']] = gt_captions\n","  \n","\n","  avg_score, scores = spice_metric.compute_score(references, hypotheses)\n","\n","  sorted_keys = list(sorted(hypotheses.keys()))\n","  sorted_indexes = {sorted_keys[i]:i for i in range(len(sorted_keys))}\n","\n","\n","  all_spice_scores = []\n","\n","  for _, row in spice_df.iterrows():\n","    idx = sorted_indexes[row['target_caption'] + '_' + row['gt_captions']]\n","    all_spice_scores.append(float(scores[idx]['All']['f']))\n","  \n","  spice_df['target_caption_text'] = all_hypoth_captions\n","  spice_df['gt_caption_text'] = all_gt_captions\n","  spice_df['spice_score'] = all_spice_scores\n","  spice_df.to_csv('gdrive/Shareddrives/CS2952N/human_preference.csv', index=False)\n","  return avg_score\n","\n","\n","def compute_all_cider_scores(caption_dictionary, experiment='test'):\n","  \n","  cider_df = pd.read_csv('gdrive/Shareddrives/CS2952N/human_preference.csv')\n","\n","  references = {}\n","  hypotheses = {}\n","\n","  all_cider_scores = []\n","  all_hypoth_captions = []\n","  all_gt_captions = []\n","\n","\n","  for idx, row in cider_df.iterrows():\n","\n","    if row['image_filename'] not in expert_annotations:\n","      continue\n","\n","    hypoth_caption = caption_dictionary[row['target_caption']][2]\n","    gt_captions = caption_dictionary[row['gt_captions']]\n","\n","    all_hypoth_captions.append(hypoth_caption)\n","    all_gt_captions.append(';'.join(gt_captions))\n","\n","\n","    hypotheses[row['target_caption']+'_'+row['gt_captions']] = [hypoth_caption]\n","    references[row['target_caption']+'_'+row['gt_captions']] = gt_captions\n","  \n","\n","  avg_score, scores = cider_metric.compute_score(references, hypotheses)\n","\n","  sorted_keys = list(sorted(hypotheses.keys()))\n","  sorted_indexes = {sorted_keys[i]:i for i in range(len(sorted_keys))}\n","\n","\n","  all_cider_scores = []\n","\n","  for _, row in cider_df.iterrows():\n","    idx = sorted_indexes[row['target_caption'] + '_' + row['gt_captions']]\n","    all_cider_scores.append(scores[idx])\n","  \n","  cider_df['target_caption_text'] = all_hypoth_captions\n","  cider_df['gt_caption_text'] = all_gt_captions\n","  cider_df['cider_score'] = all_cider_scores\n","  cider_df.to_csv('gdrive/Shareddrives/CS2952N/human_preference.csv',index=False)\n","  return avg_score"]},{"cell_type":"code","source":["def compute_all_clip_scores1(image_dataloader, batch_size, model, experiment='test'):\n","  average_clip_score = 0\n","\n","  \n","\n","  cosine_sim = torch.nn.CosineSimilarity(dim=1)\n","\n","  clip_df = pd.read_csv('gdrive/Shareddrives/CS2952N/human_preference.csv')\n","\n","\n","  all_clip_scores = []\n","  \n","\n","  for idx, (image_filenames, image_features, captions) in enumerate(image_dataloader):\n","    \n","    \n","    print('Processing image {} of {}'.format(idx+1, len(image_dataloader)))\n","    \n","    refs = np.transpose(np.array(captions)).flatten()\n","    captions_per_image = np.array(captions).shape[0]\n","\n","    \n","    text_tokens = clip.tokenize(refs).cuda()\n","\n","    with torch.no_grad():\n","      text_features = model.encode_text(text_tokens).float()\n","      text_features /= text_features.norm(dim=-1, keepdim=True)\n","\n","    image_features = image_features.repeat(1, captions_per_image, 1)  \n","\n","    image_features = torch.reshape(image_features, (image_features.shape[0]*image_features.shape[1], image_features.shape[2]))\n","    \n","    \n","    \n","    cos_sim = cosine_sim(text_features, image_features)\n","    cos_sim[cos_sim<0] = 0\n","\n","    clip_scores = 2.5*cos_sim\n","\n","\n","    # extract all values (for a batch) that are to be added to pandas df\n","    clip_scores = clip_scores.cpu().numpy()\n","\n","\n","    \n","    all_clip_scores += list(clip_scores)\n","    average_clip_score += np.sum(clip_scores)\n","\n","  clip_df['clip_score'] = all_clip_scores\n","\n","  clip_df.to_csv('gdrive/Shareddrives/CS2952N/human_preference.csv', index=False)\n","  return average_clip_score/(len(image_dataloader)*batch_size)"],"metadata":{"id":"PbobQarVlUcp","executionInfo":{"status":"ok","timestamp":1683595148469,"user_tz":240,"elapsed":121,"user":{"displayName":"collab usage7","userId":"12719994298090451758"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["f = open(os.path.relpath('gdrive/Shareddrives/CS2952N/Flickr8k_text/ExpertAnnotations.txt'), 'r')\n","\n","expert_annotations = set()\n","\n","file_exists = os.path.isfile('gdrive/Shareddrives/CS2952N/human_preference.csv')\n","\n","if file_exists:\n","  human_preference_df = pd.read_csv('gdrive/Shareddrives/CS2952N/human_preference.csv')\n","else:\n","  human_preference_df = pd.DataFrame(columns=['image_filename','target_caption','gt_captions'])\n","\n","for line in tqdm(f.readlines()):\n","  \n","  data_line = line.split('\\n')[0].split('\\t')\n","  expert_annotations.add(data_line[0])\n","  expert_annotations.add(data_line[1].split('#')[0])\n","\n","  if not file_exists:\n","    human_preference_df.loc[len(human_preference_df.index)] = [data_line[0], data_line[1].split('#')[0], data_line[0]] \n","\n","if not file_exists:\n","  human_preference_df.to_csv('gdrive/Shareddrives/CS2952N/human_preference.csv', index=False)\n","\n","print('\\nExpert Annotations Length: {} \\n'.format(len(expert_annotations)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZLiE9G2AA_sY","executionInfo":{"status":"ok","timestamp":1683594278480,"user_tz":240,"elapsed":1983,"user":{"displayName":"collab usage7","userId":"12719994298090451758"}},"outputId":"b4746e5b-cb5e-4c87-b23c-2ba09a7e90e3"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 5822/5822 [00:00<00:00, 538913.27it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Expert Annotations Length: 1000 \n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["f = open(os.path.relpath('gdrive/Shareddrives/CS2952N/Flickr8k_text/Flickr8k.token.txt'), 'r')\n","\n","caption_dictionary = {}\n","\n","for line in tqdm(f.readlines()):\n","\n","  if line[:line.index('#')] not in expert_annotations:\n","    continue\n","\n","  if line[:line.index('#')] not in caption_dictionary:\n","    caption_dictionary[line[:line.index('#')]] = [line[line.index('#')+3:].strip().lower()]\n","  else:\n","    caption_dictionary[line[:line.index('#')]].append(line[line.index('#')+3:].strip().lower())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8M5ldcWG607i","executionInfo":{"status":"ok","timestamp":1683594278958,"user_tz":240,"elapsed":479,"user":{"displayName":"collab usage7","userId":"12719994298090451758"}},"outputId":"8a42e88d-bc13-4cfa-cf2b-58fe430a0484"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 40460/40460 [00:00<00:00, 753422.07it/s]\n"]}]},{"cell_type":"code","execution_count":12,"metadata":{"id":"zfJK8EqtgPUv","executionInfo":{"status":"ok","timestamp":1683594642037,"user_tz":240,"elapsed":115,"user":{"displayName":"collab usage7","userId":"12719994298090451758"}}},"outputs":[],"source":["class FlickrDataset(torch.utils.data.Dataset):\n","\n","  def __init__(self, human_preference_df, caption_dictionary, clip_model, image_preprocessor):\n","\n","    self.BASE_PATH = 'gdrive/Shareddrives/CS2952N/Flicker8k_Dataset'\n","\n","    self.human_pref_df = human_preference_df\n","    self.caption_dict = caption_dictionary\n","    self.clip_model = clip_model\n","    self.image_preprocessor = image_preprocessor\n","  \n","  def image_load_preprocess_embed(self, filename):\n","\n","    image = Image.open(os.path.join(self.BASE_PATH, filename)).convert(\"RGB\")\n","    image = self.image_preprocessor(image)\n","\n","    \n","    image = torch.unsqueeze(image, dim=0).cuda()\n","\n","    with torch.no_grad():\n","      image_embedding = self.clip_model.encode_image(image).float()\n","    image_embedding /= image_embedding.norm(dim=-1, keepdim=True)\n","    return image_embedding\n","  \n","  def __len__(self):\n","    return len(self.human_pref_df)\n","  \n","  def __getitem__(self, index):\n","    \n","    row = self.human_pref_df.iloc[index]\n","    image_features = self.image_load_preprocess_embed(row['image_filename'])\n","    captions = ['A photo depicts ' + self.caption_dict[row['target_caption']][2]]\n","    \n","   \n","    \n","\n","    return row['image_filename'], image_features, captions"]},{"cell_type":"markdown","source":["#CLIP Scores"],"metadata":{"id":"iO7_VvmcS7jY"}},{"cell_type":"code","source":["dataset = FlickrDataset(human_preference_df, caption_dictionary, clip_model, preprocessor)\n","image_dataloader = torch.utils.data.DataLoader(dataset, batch_size=50)\n","\n","avg_clip_scores = compute_all_clip_scores1(image_dataloader, 50, clip_model, experiment='flickr8k')\n","print(avg_clip_scores)"],"metadata":{"id":"OHRElnfPS-Eq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683595663810,"user_tz":240,"elapsed":494912,"user":{"displayName":"collab usage7","userId":"12719994298090451758"}},"outputId":"d6e61b2b-ac34-4dda-e866-b0d832690dda"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing image 1 of 117\n","Processing image 2 of 117\n","Processing image 3 of 117\n","Processing image 4 of 117\n","Processing image 5 of 117\n","Processing image 6 of 117\n","Processing image 7 of 117\n","Processing image 8 of 117\n","Processing image 9 of 117\n","Processing image 10 of 117\n","Processing image 11 of 117\n","Processing image 12 of 117\n","Processing image 13 of 117\n","Processing image 14 of 117\n","Processing image 15 of 117\n","Processing image 16 of 117\n","Processing image 17 of 117\n","Processing image 18 of 117\n","Processing image 19 of 117\n","Processing image 20 of 117\n","Processing image 21 of 117\n","Processing image 22 of 117\n","Processing image 23 of 117\n","Processing image 24 of 117\n","Processing image 25 of 117\n","Processing image 26 of 117\n","Processing image 27 of 117\n","Processing image 28 of 117\n","Processing image 29 of 117\n","Processing image 30 of 117\n","Processing image 31 of 117\n","Processing image 32 of 117\n","Processing image 33 of 117\n","Processing image 34 of 117\n","Processing image 35 of 117\n","Processing image 36 of 117\n","Processing image 37 of 117\n","Processing image 38 of 117\n","Processing image 39 of 117\n","Processing image 40 of 117\n","Processing image 41 of 117\n","Processing image 42 of 117\n","Processing image 43 of 117\n","Processing image 44 of 117\n","Processing image 45 of 117\n","Processing image 46 of 117\n","Processing image 47 of 117\n","Processing image 48 of 117\n","Processing image 49 of 117\n","Processing image 50 of 117\n","Processing image 51 of 117\n","Processing image 52 of 117\n","Processing image 53 of 117\n","Processing image 54 of 117\n","Processing image 55 of 117\n","Processing image 56 of 117\n","Processing image 57 of 117\n","Processing image 58 of 117\n","Processing image 59 of 117\n","Processing image 60 of 117\n","Processing image 61 of 117\n","Processing image 62 of 117\n","Processing image 63 of 117\n","Processing image 64 of 117\n","Processing image 65 of 117\n","Processing image 66 of 117\n","Processing image 67 of 117\n","Processing image 68 of 117\n","Processing image 69 of 117\n","Processing image 70 of 117\n","Processing image 71 of 117\n","Processing image 72 of 117\n","Processing image 73 of 117\n","Processing image 74 of 117\n","Processing image 75 of 117\n","Processing image 76 of 117\n","Processing image 77 of 117\n","Processing image 78 of 117\n","Processing image 79 of 117\n","Processing image 80 of 117\n","Processing image 81 of 117\n","Processing image 82 of 117\n","Processing image 83 of 117\n","Processing image 84 of 117\n","Processing image 85 of 117\n","Processing image 86 of 117\n","Processing image 87 of 117\n","Processing image 88 of 117\n","Processing image 89 of 117\n","Processing image 90 of 117\n","Processing image 91 of 117\n","Processing image 92 of 117\n","Processing image 93 of 117\n","Processing image 94 of 117\n","Processing image 95 of 117\n","Processing image 96 of 117\n","Processing image 97 of 117\n","Processing image 98 of 117\n","Processing image 99 of 117\n","Processing image 100 of 117\n","Processing image 101 of 117\n","Processing image 102 of 117\n","Processing image 103 of 117\n","Processing image 104 of 117\n","Processing image 105 of 117\n","Processing image 106 of 117\n","Processing image 107 of 117\n","Processing image 108 of 117\n","Processing image 109 of 117\n","Processing image 110 of 117\n","Processing image 111 of 117\n","Processing image 112 of 117\n","Processing image 113 of 117\n","Processing image 114 of 117\n","Processing image 115 of 117\n","Processing image 116 of 117\n","Processing image 117 of 117\n","0.38699279540624376\n"]}]},{"cell_type":"markdown","metadata":{"id":"mLMd1rREjTBh"},"source":["#BLEU Score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12527,"status":"ok","timestamp":1683586654455,"user":{"displayName":"Shreyas Sundara Raman","userId":"16214783268136716971"},"user_tz":240},"id":"VBjwDxgLjSPw","outputId":"89127683-6ec3-4272-f3c2-bcfeb000f069"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:515: UserWarning: \n","The hypothesis contains 0 counts of 3-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:515: UserWarning: \n","The hypothesis contains 0 counts of 4-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:515: UserWarning: \n","The hypothesis contains 0 counts of 2-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n"]},{"output_type":"stream","name":"stdout","text":["[0.42205383560655263, 0.21408994031667053, 0.13163341355074143, 0.09072776977871559]\n"]}],"source":["avg_bleu_scores = compute_all_bleu_scores(caption_dictionary, experiment='flickr8k')\n","print(avg_bleu_scores)"]},{"cell_type":"markdown","metadata":{"id":"pAM9N9aUDGtf"},"source":["#METEOR Score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39692,"status":"ok","timestamp":1683589987466,"user":{"displayName":"Shreyas Sundara Raman","userId":"16214783268136716971"},"user_tz":240},"id":"fVCSBRGQDKmy","outputId":"7afc6921-bd61-4b4c-c316-c58e72281345"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.26462198442989143\n"]}],"source":["avg_meteor_scores = compute_all_meteor_scores(caption_dictionary, experiment='flickr8k')\n","print(avg_meteor_scores)"]},{"cell_type":"markdown","metadata":{"id":"gxsWGNSAZ9fQ"},"source":["#SPICE Score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2045895,"status":"ok","timestamp":1683593523749,"user":{"displayName":"Shreyas Sundara Raman","userId":"16214783268136716971"},"user_tz":240},"id":"7ejd7ZO_azGl","outputId":"f2cf9ca3-80b3-42f5-fd11-c4cf76f42d28"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.06270574520426106\n"]}],"source":["avg_spice_scores = compute_all_spice_scores(caption_dictionary, experiment='flickr8k')\n","print(avg_spice_scores)"]},{"cell_type":"code","source":["print(avg_spice_scores)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2jRIyfxqDVBE","executionInfo":{"status":"ok","timestamp":1683527399614,"user_tz":240,"elapsed":5,"user":{"displayName":"Shreyas Sundara Raman","userId":"16214783268136716971"}},"outputId":"c7f2a38a-bd58-4828-ab41-6cd3fbc98349"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.2305360597108907\n"]}]},{"cell_type":"markdown","source":["#CIDER Score"],"metadata":{"id":"to_vgd8Lc7bx"}},{"cell_type":"code","source":["avg_cider_scores = compute_all_cider_scores(caption_dictionary, experiment='flickr8k')\n","print(avg_cider_scores)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"llFK-Ctcc9d-","executionInfo":{"status":"ok","timestamp":1683590786818,"user_tz":240,"elapsed":10945,"user":{"displayName":"Shreyas Sundara Raman","userId":"16214783268136716971"}},"outputId":"5367d458-cdf9-44a9-c511-b609c8c5b2b7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.18337528908922313\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["iO7_VvmcS7jY","mLMd1rREjTBh","to_vgd8Lc7bx"],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}